# -*- coding: utf-8 -*-
"""Deborina_tokenization_tutorial.ipynb"

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TdiyMSa-zcuy-aND65VAG-6dOBudYyGn

# Токенизация в NLP

## Введение

**Токенизация** – это процесс разбиения текста на минимальные единицы (токены): слова, подслова или символы.  
Она является первым шагом во многих NLP задачах: анализ тональности, машинный перевод, чат-боты, обучение языковых моделей.

В этом туториале мы разберём разные виды токенизации и библиотеки, которые их реализуют.

## 1. Простые методы токенизации

### 1.1. Разделение по пробелам
"""

text = "Hello, world! This is a test."
text.split()

"""
### 1.2. Регулярные выражения
"""

import re

text = "Hello, world! This is a test."
tokens = re.findall(r"\w+", text)
tokens

"""**Ограничения простых методов**:  
- Не учитывают пунктуацию.  
- Не работают с разными языками и морфологией.  
- Не подходят для серьёзных задач.

## 2. Токенизация с помощью библиотек

### 2.1. NLTK  
**Применение**: образовательные проекты, простая предобработка текста.  
**Ограничения**: работает медленно на больших корпусах.  
**Когда использовать**: для курсовых и лабораторных работ по основам NLP.
"""

import nltk
nltk.download('punkt')
nltk.download('punkt_tab')

from nltk.tokenize import word_tokenize, sent_tokenize

text = "Hello, world! This is a test."
print(word_tokenize(text))
print(sent_tokenize(text))

"""
### 2.2. spaCy  
**Применение**: промышленные проекты, морфология, синтаксис.  
**Ограничения**: большие модели, требует ресурсов.  
**Когда использовать**: для курсовых по синтаксису и морфологии, проектов по обработке естественного языка.
"""

import spacy

nlp = spacy.load("en_core_web_sm")
doc = nlp("Hello, world! This is a test.")
[t.text for t in doc]

"""
### 2.3. Stanza  
**Применение**: многоязычные корпуса, морфология, синтаксис.  
**Ограничения**: медленнее spaCy, требует загрузки моделей.  
**Когда использовать**: в исследованиях и при работе с редкими языками.
"""

!pip install stanza
import stanza

stanza.download("en")
nlp = stanza.Pipeline("en")
doc = nlp("Hello, world! This is a test.")
[[word.text for word in sent.words] for sent in doc.sentences]

"""
### 2.4. MosesTokenizer (sacremoses)  
**Применение**: машинный перевод, предобработка текстов.  
**Ограничения**: устаревающий стандарт.  
**Когда использовать**: для экспериментов с классическими MT-системами.
"""

!pip install sacremoses

from sacremoses import MosesTokenizer

mt = MosesTokenizer()
mt.tokenize("Hello, world! This is a test.")

"""
### 2.5. gensim  
**Применение**: подготовка данных для моделей `word2vec`, `fastText`.  
**Ограничения**: базовая токенизация.  
**Когда использовать**: при обучении собственных векторных моделей.  
"""

!pip install gensim
from gensim.utils import simple_preprocess

simple_preprocess("Hello, world! This is a test.")

"""
### 2.6. HuggingFace Tokenizers  
**Применение**: трансформеры, нейросетевые модели.  
**Ограничения**: сложность, нужен GPU для обучения токенизаторов.  
**Когда использовать**: в курсовых и проектах с BERT, GPT, T5.
"""

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
tokens = tokenizer("Hello, world! This is a test.")
tokens

"""
## 3. Посимвольная токенизация  
"""

list("Hello, world!")

"""## 4. Комбинированные методы  

- **Послоговая токенизация** (актуально для языков с чёткой слоговой структурой, например, японский).  
- **Гибридные методы**: совмещение слов и символов.  

Пример: статья *Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models*.

## 5. Сравнение токенизаторов  

Попробуем применить разные методы к одному и тому же тексту.

## 6. Практика

Возьмём новостной текст, токенизируем разными способами и сравним.
"""

article = 'Natural language processing (NLP) is a field of artificial intelligence\
that gives computers the ability to understand text and spoken words in much the same way human beings can.'

"""
NLTK
"""

from nltk.tokenize import word_tokenize, sent_tokenize

print(word_tokenize(article))

"""
SpaCy
"""

import spacy

nlp = spacy.load("en_core_web_sm")
doc = nlp(article)
print([t.text for t in doc])

"""
Gensim
"""

from gensim.utils import simple_preprocess

print(simple_preprocess(article))

"""## 7. Задача для самостоятельного разбора  

Прочитать статью:  
**"Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models"** (https://arxiv.org/abs/1604.00788)

### Напишите ответы на вопросы:
1. Что значит Out-of-Vocabulary?  
2. Как эту проблемы решили авторы статьи "Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models"?
3. Какие еще типы токенизации мы разбирали на занятии?

Ваш ответ здесь:

1. Out-of-Vocabulary (OOV) — это слова, которых нет в словаре модели. В нейронном машинном переводе (NMT) обычные модели имеют ограниченный словарь (например, 50 000 самых частотных слов), и все остальные заменяются на специальный символ <unk>.
Из-за этого система теряет информацию о редких или новых словах b не способна переводить неологизмы, имена собственные, формы слов, заимствования и т. д.
2. Авторы предложили гибридную модель — Hybrid Word–Character NMT, которая совмещает преимущества словарного и посимвольного подходов: основная часть перевода выполняется на уровне слов (word-level NMT) — это быстрее и стабильнее. Редкие слова (OOV) обрабатываются посимвольно (character-level).
3. Токенизация на основе пробелов и знаков препинания; подсловная токенизация (BPE(Byte Pair Encoding), WordPiece) — разбиение на часто встречающиеся части.
"""